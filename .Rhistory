testDb$OveralGradeDiff=NULL
testDb[is.na(testDb)]=0
#---- use trained model to predict progress for test data
preds= predict(model3, newdata=testDb[,fs]);
#========================================================================
#         step 2.1: prepare submission file for kaggle
#========================================================================
cl.Results=testDb[,c('ProblemID', 'UserID')]
cl.Results$overalGradeDiff=preds
cl.Results$uniqRowID= paste0(cl.Results$UserID,'_', cl.Results$ProblemID)
cl.Results=cl.Results[,c('uniqRowID','overalGradeDiff')]
table(cl.Results$overalGradeDiff)
#----- keep only rows which are listed in classifier_templtae.csv file
#----- this excludes first submissions and cases with no forum and video event in between two submissions
regression_template= read.csv('regression_template.csv', stringsAsFactors = F)
kaggleSubmission=merge(regression_template,cl.Results )
kaggleSubmission$overalGradeDiff[kaggleSubmission$overalGradeDiff>100] <- 100
write.csv(kaggleSubmission,file='regression_results.csv', row.names = F)
linearModel <- train(
y=db.train$overalGradeDiff,
x=db.train[,fsNorm],
trControl = ctrl,
metric = "RMSE",
method = "lm"
)
linearModel
fs = c(
"countOfVideoEvents",
#"countOfForumEvents",
"ProblemID",
"totalTime",
"totalVideoTime",
#"NoOfVidoesWatched",
"countOfSubmissions",
"countOfThreadViews"
)
#check correlation
correlation_matrix <- cor(db.train[,fs])
corrplot(correlation_matrix, method = "number")
#check correlation
correlation_matrix <- cor(db.train[,fsNorm])
corrplot(correlation_matrix, method = "number")
fsNorm =c(
#"VideoPerSubmission",
#"ForumPerSubmission",
"ProblemID",
"totalTime",
"NoOfVidoesWatchedPerSubmission",
"VideoTimePerSubmission",
"ThreadViewPerSubmission",
"countOfSubmissions"
)
#check correlation
correlation_matrix <- cor(db.train[,fsNorm])
corrplot(correlation_matrix, method = "number")
View(db)
fsNorm =c(
#"VideoPerSubmission",
#"ForumPerSubmission",
"ProblemID",
"totalTime",
#"NoOfVidoesWatchedPerSubmission",
"VideoTimePerSubmission",
"ThreadViewPerSubmission",
"countOfSubmissions"
)
#check correlation
correlation_matrix <- cor(db.train[,fsNorm])
corrplot(correlation_matrix, method = "number")
#for normalized features
fsNorm =c(
"VideoPerSubmission",
#"ForumPerSubmission",
"ProblemID",
"totalTime",
#"NoOfVidoesWatchedPerSubmission",
"VideoTimePerSubmission",
"ThreadViewPerSubmission",
"countOfSubmissions"
)
#check correlation
correlation_matrix <- cor(db.train[,fsNorm])
corrplot(correlation_matrix, method = "number")
#Control function
set.seed(123)
ctrl <- trainControl(method = "repeatedcv", repeats =3, number = 10)
#lm
linearModel <- train(
y=db.train$overalGradeDiff,
x=db.train[,fsNorm],
trControl = ctrl,
metric = "RMSE",
method = "lm"
)
linearModel
fs = c(
"countOfVideoEvents",
#"countOfForumEvents",
"ProblemID",
"totalTime",
"totalVideoTime",
#"NoOfVidoesWatched",
"countOfSubmissions",
"countOfThreadViews"
)
#for normalized features
fsNorm =c(
"VideoPerSubmission",
#"ForumPerSubmission",
"ProblemID",
"totalTime",
#"NoOfVidoesWatchedPerSubmission",
"VideoTimePerSubmission",
"ThreadViewPerSubmission",
"countOfSubmissions"
)
#check correlation
correlation_matrix <- cor(db.train[,fs])
corrplot(correlation_matrix, method = "number")
#glmnet
model4<- train(y=db.train$overalGradeDiff,
x = db.train[,fsNorm],
method="glmnet",
trControl = ctrl,
tuneGrid = expand.grid(alpha = (1:10) * 0.1, lambda = (1:10) * 0.1),
metric = "RMSE",
preProc = c("center", "scale"))
model4
testDb=read.csv('features_test.csv', stringsAsFactors = F)
testDb$OveralGradeDiff=NULL
testDb[is.na(testDb)]=0
#---- use trained model to predict progress for test data
preds= predict(model4, newdata=testDb[,fs]);
#========================================================================
#         step 2.1: prepare submission file for kaggle
#========================================================================
cl.Results=testDb[,c('ProblemID', 'UserID')]
cl.Results$overalGradeDiff=preds
cl.Results$uniqRowID= paste0(cl.Results$UserID,'_', cl.Results$ProblemID)
cl.Results=cl.Results[,c('uniqRowID','overalGradeDiff')]
table(cl.Results$overalGradeDiff)
#----- keep only rows which are listed in classifier_templtae.csv file
#----- this excludes first submissions and cases with no forum and video event in between two submissions
regression_template= read.csv('regression_template.csv', stringsAsFactors = F)
kaggleSubmission=merge(regression_template,cl.Results )
kaggleSubmission$overalGradeDiff[kaggleSubmission$overalGradeDiff>100] <- 100
write.csv(kaggleSubmission,file='regression_results.csv', row.names = F)
testDb=read.csv('features_test.csv', stringsAsFactors = F)
testDb$OveralGradeDiff=NULL
testDb[is.na(testDb)]=0
#---- use trained model to predict progress for test data
preds= predict(model4, newdata=testDb[,fs]);
testDb=read.csv('features_test.csv', stringsAsFactors = F)
testDb$OveralGradeDiff=NULL
testDb[is.na(testDb)]=0
#---- use trained model to predict progress for test data
preds= predict(model4, newdata=testDb[,fsNorm]);
#========================================================================
#         step 2.1: prepare submission file for kaggle
#========================================================================
cl.Results=testDb[,c('ProblemID', 'UserID')]
cl.Results$overalGradeDiff=preds
cl.Results$uniqRowID= paste0(cl.Results$UserID,'_', cl.Results$ProblemID)
cl.Results=cl.Results[,c('uniqRowID','overalGradeDiff')]
table(cl.Results$overalGradeDiff)
#----- keep only rows which are listed in classifier_templtae.csv file
#----- this excludes first submissions and cases with no forum and video event in between two submissions
regression_template= read.csv('regression_template.csv', stringsAsFactors = F)
kaggleSubmission=merge(regression_template,cl.Results )
kaggleSubmission$overalGradeDiff[kaggleSubmission$overalGradeDiff>100] <- 100
write.csv(kaggleSubmission,file='regression_results.csv', row.names = F)
model4$finalModel
model4
#for normalized features
fsNorm =c(
"VideoPerSubmission",
"ForumPerSubmission",
"ProblemID",
"totalTime",
#"NoOfVidoesWatchedPerSubmission",
"VideoTimePerSubmission",
#"ThreadViewPerSubmission",
"countOfSubmissions"
)
model4<- train(y=db.train$overalGradeDiff,
x = db.train[,fsNorm],
method="glmnet",
trControl = ctrl,
tuneGrid = expand.grid(alpha = (1:10) * 0.1, lambda = (1:10) * 0.1),
metric = "RMSE",
preProc = c("center", "scale"))
model4
library(dplyr)
library(plyr) #ddply
#------ read data frame
db=read.csv('datasets/OutputTable.csv')
#------ sort submissions
db=db[order(db$UserID,db$ProblemID,db$SubmissionNumber),]
# dim(db)
# View(db)
#------- aggregate by UserID and ProblemID ---------
length(unique(db$UserID))
agg.features=ddply(db, .(UserID,ProblemID), summarise,
overalGradeDiff=Grade[length(Grade)]-Grade[1],
countOfSubmissions=length(SubmissionNumber),
totalTime = sum(TimeSinceLast, na.rm = T),
totalVideoTime= sum(DurationOfVideoActivity,na.rm=T),
countOfVideoEvents = sum(NVideoEvents,na.rm = T),
countOfForumEvents = sum(NForumEvents,na.rm = T),
countOfThreadViews = sum(NumberOfThreadViews, na.rm=T),
NoOfVidoesWatched = sum(NoOfVidoesWatched, na.rm = T))
#-------normalize on submissions---------
agg.features["VideoPerSubmission"]<-NA
agg.features$VideoPerSubmission<-(agg.features$countOfVideoEvents/agg.features$countOfSubmissions)
agg.features["ForumPerSubmission"]<-NA
agg.features$ForumPerSubmission<-(agg.features$countOfForumEvents/agg.features$countOfSubmissions)
agg.features["VideoTimePerSubmission"]<-NA
agg.features$VideoTimePerSubmission<-(agg.features$totalVideoTime/agg.features$countOfSubmissions)
agg.features["ThreadViewPerSubmission"]<-NA
agg.features$ThreadViewPerSubmission<-(agg.features$countOfThreadViews/agg.features$countOfSubmissions)
agg.features["NoOfVidoesWatchedPerSubmission"]<-NA
agg.features$NoOfVidoesWatchedPerSubmission <-(agg.features$NoOfVidoesWatched/
agg.features$countOfSubmissions)
agg.features[is.na(agg.features)] <- 0
#------ remove cases with only one attempt
agg.features=filter(agg.features,countOfSubmissions>1); dim(agg.features)
#------ save feature file
write.csv(agg.features, file='features.csv')
#SAME FOR TEST
#------ read data frame
db=read.csv('datasets/OutputTable_test.csv')
#------ sort submissions
db=db[order(db$UserID,db$ProblemID,db$SubmissionNumber),]
# dim(db)
# View(db)
#------- aggregate by UserID and ProblemID ---------
length(unique(db$UserID))
agg.features=ddply(db, .(UserID,ProblemID), summarise,
overalGradeDiff=Grade[length(Grade)]-Grade[1],
countOfSubmissions=length(SubmissionNumber),
totalTime = sum(TimeSinceLast, na.rm = T),
totalVideoTime= sum(DurationOfVideoActivity,na.rm=T),
countOfVideoEvents = sum(NVideoEvents,na.rm = T),
countOfForumEvents = sum(NForumEvents,na.rm = T),
countOfThreadViews = sum(NumberOfThreadViews, na.rm=T),
NoOfVidoesWatched = sum(NoOfVidoesWatched, na.rm = T))
#-------normalize on submissions---------
agg.features["VideoPerSubmission"]<-NA
agg.features$VideoPerSubmission<-(agg.features$countOfVideoEvents/agg.features$countOfSubmissions)
agg.features["ForumPerSubmission"]<-NA
agg.features$ForumPerSubmission<-(agg.features$countOfForumEvents/agg.features$countOfSubmissions)
agg.features["VideoTimePerSubmission"]<-NA
agg.features$VideoTimePerSubmission<-(agg.features$totalVideoTime/agg.features$countOfSubmissions)
agg.features["ThreadViewPerSubmission"]<-NA
agg.features$ThreadViewPerSubmission<-(agg.features$countOfThreadViews/agg.features$countOfSubmissions)
agg.features["NoOfVidoesWatchedPerSubmission"]<-NA
agg.features$NoOfVidoesWatchedPerSubmission <-(agg.features$NoOfVidoesWatched/
agg.features$countOfSubmissions)
agg.features[is.na(agg.features)] <- 0
#------ remove cases with only one attempt
agg.features=filter(agg.features,countOfSubmissions>1); dim(agg.features)
#------ save feature file
write.csv(agg.features, file='features_test.csv')
#------ read features extracted from train set, using your python script
db=read.csv('features.csv', stringsAsFactors = F)
#------ sort submissions
db=db[order(db$UserID,db$ProblemID),]
#--- replace NA values with 0
db[is.na(db)]=0
# ----- (Optional) split your training data into train and test set. Use train set to build your classifier and try it on test data to check generalizability.
set.seed(1234)
tr.index= sample(nrow(db), nrow(db)*0.99)
db.train= db[tr.index,]
db.test = db[-tr.index,]
dim(db.train)
dim(db.test)
library(caret)
fs = c(
"countOfVideoEvents",
#"countOfForumEvents",
"ProblemID",
"totalTime",
"totalVideoTime",
#"NoOfVidoesWatched",
"countOfSubmissions",
"countOfThreadViews"
)
#for normalized features
fsNorm =c(
"VideoPerSubmission",
"ForumPerSubmission",
"ProblemID",
"totalTime",
#"NoOfVidoesWatchedPerSubmission",
"VideoTimePerSubmission",
#"ThreadViewPerSubmission",
"countOfSubmissions"
)
linearModel <- train(
y=db.train$overalGradeDiff,
x=db.train[,fsNorm],
trControl = ctrl,
metric = "RMSE",
method = "lm"
)
linearModel
#check correlation
correlation_matrix <- cor(db.train[,fsNorm])
corrplot(correlation_matrix, method = "number")
#for normalized features
fsNorm =c(
"VideoPerSubmission",
"ForumPerSubmission",
"ProblemID",
"totalTime",
"NoOfVidoesWatchedPerSubmission",
"VideoTimePerSubmission",
"ThreadViewPerSubmission",
"countOfSubmissions"
)
correlation_matrix <- cor(db.train[,fsNorm])
corrplot(correlation_matrix, method = "number")
View(db)
fsNorm =c(
#"VideoPerSubmission",
#"ForumPerSubmission",
"ProblemID",
"totalTime",
"NoOfVidoesWatchedPerSubmission",
"VideoTimePerSubmission",
"ThreadViewPerSubmission",
"countOfSubmissions"
)
#Control function
set.seed(123)
ctrl <- trainControl(method = "repeatedcv", repeats =3, number = 10)
#lm
linearModel <- train(
y=db.train$overalGradeDiff,
x=db.train[,fsNorm],
trControl = ctrl,
metric = "RMSE",
method = "lm"
)
linearModel
linearModel$bestTune
linearModel$finalModel
fs = c(
#"countOfVideoEvents",
#"countOfForumEvents",
"ProblemID",
"totalTime",
"totalVideoTime",
"NoOfVidoesWatched",
"countOfSubmissions",
"countOfThreadViews"
)
#cubist
cubist<- train(y=db.train$overalGradeDiff,
x=db.train[,fs],
method= "cubist",
trControl=ctrl,
tuneGrid = expand.grid(committees = c(21,22,23), neighbors = c(9)),
metric="RMSE",
preProc= c("center", "scale"))
cubist
varImp(cubist)
correlation_matrix <- cor(db.train[,fs])
corrplot(correlation_matrix, method = "number")
fs = c(
"countOfVideoEvents",
#"countOfForumEvents",
"ProblemID",
"totalTime",
"totalVideoTime",
"NoOfVidoesWatched",
"countOfSubmissions",
"countOfThreadViews"
)
#check correlation
correlation_matrix <- cor(db.train[,fs])
corrplot(correlation_matrix, method = "number")
cubist<- train(y=db.train$overalGradeDiff,
x=db.train[,fs],
method= "cubist",
trControl=ctrl,
tuneGrid = expand.grid(committees = c(20,21,22,23,24), neighbors = c(9)),
metric="RMSE",
preProc= c("center", "scale"))
cubist
model3
#cubist
cubist<- train(y=db.train$overalGradeDiff,
x=db.train[,fs],
method= "cubist",
trControl=ctrl,
tuneGrid = expand.grid(committees = c(23,24,25), neighbors = c(9)),
metric="RMSE",
preProc= c("center", "scale"))
cubist
model3
fsNorm =c(
#"VideoPerSubmission",
#"ForumPerSubmission",
"ProblemID",
"totalTime",
"NoOfVidoesWatchedPerSubmission",
"VideoTimePerSubmission",
"ThreadViewPerSubmission",
"countOfSubmissions"
)
#glmnet
glmnet<- train(y=db.train$overalGradeDiff,
x = db.train[,fsNorm],
method="glmnet",
trControl = ctrl,
tuneGrid = expand.grid(alpha = (1:10) * 0.1, lambda = (1:10) * 0.1),
metric = "RMSE",
preProc = c("center", "scale"))
glmnet
testDb=read.csv('features_test.csv', stringsAsFactors = F)
testDb$OveralGradeDiff=NULL
testDb[is.na(testDb)]=0
#---- use trained model to predict progress for test data
preds= predict(glmnet, newdata=testDb[,fsNorm]);
#========================================================================
#         step 2.1: prepare submission file for kaggle
#========================================================================
cl.Results=testDb[,c('ProblemID', 'UserID')]
cl.Results$overalGradeDiff=preds
cl.Results$uniqRowID= paste0(cl.Results$UserID,'_', cl.Results$ProblemID)
cl.Results=cl.Results[,c('uniqRowID','overalGradeDiff')]
table(cl.Results$overalGradeDiff)
#----- keep only rows which are listed in classifier_templtae.csv file
#----- this excludes first submissions and cases with no forum and video event in between two submissions
regression_template= read.csv('regression_template.csv', stringsAsFactors = F)
kaggleSubmission=merge(regression_template,cl.Results )
kaggleSubmission$overalGradeDiff[kaggleSubmission$overalGradeDiff>100] <- 100
write.csv(kaggleSubmission,file='regression_results.csv', row.names = F)
#cubist
cubist<- train(y=db.train$overalGradeDiff,
x=db.train[,fsNorm],
method= "cubist",
trControl=ctrl,
#tuneGrid = expand.grid(committees = c(23,24,25), neighbors = c(9)),
metric="RMSE",
preProc= c("center", "scale"))
cubist
fsNorm =c(
"VideoPerSubmission",
#"ForumPerSubmission",
"ProblemID",
"totalTime",
#"NoOfVidoesWatchedPerSubmission",
"VideoTimePerSubmission",
"ThreadViewPerSubmission",
"countOfSubmissions"
)
#cubist
cubist<- train(y=db.train$overalGradeDiff,
x=db.train[,fsNorm],
method= "cubist",
trControl=ctrl,
tuneGrid = expand.grid(committees = c(19,20,21,22,23,24,25), neighbors = c(9)),
metric="RMSE",
preProc= c("center", "scale"))
cubist
#cubist
cubist<- train(y=db.train$overalGradeDiff,
x=db.train[,fsNorm],
method= "cubist",
trControl=ctrl,
tuneGrid = expand.grid(committees = c(25,26,27), neighbors = c(9)),
metric="RMSE",
preProc= c("center", "scale"))
cubist
cubist<- train(y=db.train$overalGradeDiff,
x=db.train[,fsNorm],
method= "cubist",
#trControl=ctrl,
tuneGrid = expand.grid(committees = c(25:40), neighbors = c(9)),
metric="RMSE",
preProc= c("center", "scale"))
cubist
#cubist
cubist<- train(y=db.train$overalGradeDiff,
x=db.train[,fsNorm],
method= "cubist",
trControl=ctrl,
tuneGrid = expand.grid(committees = c(32,33,34), neighbors = c(9)),
metric="RMSE",
preProc= c("center", "scale"))
cubist
testDb=read.csv('features_test.csv', stringsAsFactors = F)
testDb$OveralGradeDiff=NULL
testDb[is.na(testDb)]=0
#---- use trained model to predict progress for test data
preds= predict(cubist, newdata=testDb[,fsNorm]);
#========================================================================
#         step 2.1: prepare submission file for kaggle
#========================================================================
cl.Results=testDb[,c('ProblemID', 'UserID')]
cl.Results$overalGradeDiff=preds
cl.Results$uniqRowID= paste0(cl.Results$UserID,'_', cl.Results$ProblemID)
cl.Results=cl.Results[,c('uniqRowID','overalGradeDiff')]
table(cl.Results$overalGradeDiff)
#----- keep only rows which are listed in classifier_templtae.csv file
#----- this excludes first submissions and cases with no forum and video event in between two submissions
regression_template= read.csv('regression_template.csv', stringsAsFactors = F)
kaggleSubmission=merge(regression_template,cl.Results )
kaggleSubmission$overalGradeDiff[kaggleSubmission$overalGradeDiff>100] <- 100
write.csv(kaggleSubmission,file='regression_results.csv', row.names = F)
