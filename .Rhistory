db=db[order(db$UserID,db$ProblemID),]
#--- replace NA values with 0
db[is.na(db)]=0
#---- remove cases when there is no video or forum activity between two submissions
db= filter(db, countOfForumEvents + countOfVideoEvents>0)
# ----- (Optional) split your training data into train and test set. Use train set to build your classifier and try it on test data to check generalizability.
set.seed(1234)
tr.index= sample(nrow(db), nrow(db)*1)
db.train= db[tr.index,]
db.test = db[-tr.index,]
dim(db.train)
dim(db.test)
library(caret)
fs = c(
"countOfVideoEvents",
"countOfForumEvents",
"ProblemID",
"totalTime",
"totalVideoTime",
"NoOfVidoesWatched",
"countOfSubmissions",
"countOfThreadViews"
)
#for normalized features
fsNorm =c(
#"VideoPerSubmission",
#"ForumPerSubmission",
"ProblemID",
"totalTime",
"NoOfVidoesWatchedPerSubmission",
"VideoTimePerSubmission",
"ThreadViewPerSubmission",
"countOfSubmissions"
)
#check correlation
correlation_matrix <- cor(db.train[,fs])
corrplot(correlation_matrix, method = "number")
model3<- train(y=db.train$overalGradeDiff,
x=db.train[,fs],
method= "cubist",
trControl=ctrl,
tuneGrid = expand.grid(committees = c(21,22,23), neighbors = c(8,9)),
metric="RMSE",
preProc= c("center", "scale"))
model3
fs = c(
#"countOfVideoEvents",
#"countOfForumEvents",
"ProblemID",
"totalTime",
"totalVideoTime",
"NoOfVidoesWatched",
"countOfSubmissions",
"countOfThreadViews"
)
#check correlation
correlation_matrix <- cor(db.train[,fs])
corrplot(correlation_matrix, method = "number")
#cubist
model3<- train(y=db.train$overalGradeDiff,
x=db.train[,fs],
method= "cubist",
trControl=ctrl,
tuneGrid = expand.grid(committees = c(21,22,23), neighbors = c(8,9)),
metric="RMSE",
preProc= c("center", "scale"))
model3
fs = c(
"countOfVideoEvents",
"countOfForumEvents",
"ProblemID",
"totalTime",
"totalVideoTime",
#"NoOfVidoesWatched",
"countOfSubmissions"
#"countOfThreadViews"
)
model3<- train(y=db.train$overalGradeDiff,
x=db.train[,fs],
method= "cubist",
trControl=ctrl,
tuneGrid = expand.grid(committees = c(21,22,23), neighbors = c(8,9)),
metric="RMSE",
preProc= c("center", "scale"))
model3
fs = c(
"countOfVideoEvents",
"countOfForumEvents",
"ProblemID",
"totalTime",
"totalVideoTime",
"NoOfVidoesWatched",
"countOfSubmissions",
"countOfThreadViews"
)
#check correlation
correlation_matrix <- cor(db.train[,fs])
corrplot(correlation_matrix, method = "number")
#cubist
model3<- train(y=db.train$overalGradeDiff,
x=db.train[,fs],
method= "cubist",
trControl=ctrl,
tuneGrid = expand.grid(committees = c(21,22,23), neighbors = c(8,9)),
metric="RMSE",
preProc= c("center", "scale"))
model3
View(db)
#------ read features extracted from train set, using your python script
db=read.csv('features.csv', stringsAsFactors = F)
#------ sort submissions
db=db[order(db$UserID,db$ProblemID),]
#--- replace NA values with 0
db[is.na(db)]=0
#---- remove cases when there is no video or forum activity between two submissions
db= filter(db, countOfForumEvents + countOfVideoEvents>0)
# ----- (Optional) split your training data into train and test set. Use train set to build your classifier and try it on test data to check generalizability.
set.seed(1234)
tr.index= sample(nrow(db), nrow(db)*0.99)
db.train= db[tr.index,]
db.test = db[-tr.index,]
dim(db.train)
dim(db.test)
library(caret)
fs = c(
"countOfVideoEvents",
"countOfForumEvents",
"ProblemID",
"totalTime",
"totalVideoTime",
"NoOfVidoesWatched",
"countOfSubmissions",
"countOfThreadViews"
)
#Control function
set.seed(123)
ctrl <- trainControl(method = "repeatedcv", repeats =3, number = 10)
#lm
linearModel <- train(
y=db.train$overalGradeDiff,
x=db.train[,fsNorm],
trControl = ctrl,
metric = "RMSE",
method = "lm"
)
linearModel
model1
model
varImp(model)
#lm
linearModel <- train(
y=db.train$overalGradeDiff,
x=db.train[,fsNorm],
trControl = ctrl,
metric = "RMSE",
method = "lm"
)
linearModel
#lm
linearModel <- train(
y=db.train$overalGradeDiff,
x=db.train[,fsNorm],
trControl = ctrl,
metric = "RMSE",
method = "lm"
)
linearModel
testDb=read.csv('features_test.csv', stringsAsFactors = F)
testDb$OveralGradeDiff=NULL
testDb[is.na(testDb)]=0
#---- use trained model to predict progress for test data
preds= predict(model, newdata=testDb[,fsNorm]);
#========================================================================
#         step 2.1: prepare submission file for kaggle
#========================================================================
cl.Results=testDb[,c('ProblemID', 'UserID')]
cl.Results$overalGradeDiff=preds
cl.Results$uniqRowID= paste0(cl.Results$UserID,'_', cl.Results$ProblemID)
cl.Results=cl.Results[,c('uniqRowID','overalGradeDiff')]
table(cl.Results$overalGradeDiff)
#----- keep only rows which are listed in classifier_templtae.csv file
#----- this excludes first submissions and cases with no forum and video event in between two submissions
regression_template= read.csv('regression_template.csv', stringsAsFactors = F)
kaggleSubmission=merge(regression_template,cl.Results )
kaggleSubmission$overalGradeDiff[kaggleSubmission$overalGradeDiff>100] <- 100
write.csv(kaggleSubmission,file='regression_results.csv', row.names = F)
#---- use trained model to predict progress for test data
preds= predict(model$finalModel, newdata=testDb[,fsNorm]);
#========================================================================
#         step 2.1: prepare submission file for kaggle
#========================================================================
cl.Results=testDb[,c('ProblemID', 'UserID')]
cl.Results$overalGradeDiff=preds
cl.Results$uniqRowID= paste0(cl.Results$UserID,'_', cl.Results$ProblemID)
cl.Results=cl.Results[,c('uniqRowID','overalGradeDiff')]
table(cl.Results$overalGradeDiff)
#----- keep only rows which are listed in classifier_templtae.csv file
#----- this excludes first submissions and cases with no forum and video event in between two submissions
regression_template= read.csv('regression_template.csv', stringsAsFactors = F)
kaggleSubmission=merge(regression_template,cl.Results )
kaggleSubmission$overalGradeDiff[kaggleSubmission$overalGradeDiff>100] <- 100
write.csv(kaggleSubmission,file='regression_results.csv', row.names = F)
model
# ----- (Optional) split your training data into train and test set. Use train set to build your classifier and try it on test data to check generalizability.
set.seed(1234)
tr.index= sample(nrow(db), nrow(db)*0.99)
db.train= db[tr.index,]
db.test = db[-tr.index,]
dim(db.train)
dim(db.test)
fs = c(
"countOfVideoEvents",
"countOfForumEvents",
"ProblemID",
"totalTime",
"totalVideoTime",
"NoOfVidoesWatched",
"countOfSubmissions",
"countOfThreadViews"
)
fs = c(
#"countOfVideoEvents",
#"countOfForumEvents",
"ProblemID",
"totalTime",
"totalVideoTime",
"NoOfVidoesWatched",
"countOfSubmissions",
"countOfThreadViews"
)
#for normalized features
fsNorm =c(
#"VideoPerSubmission",
#"ForumPerSubmission",
"ProblemID",
"totalTime",
"NoOfVidoesWatchedPerSubmission",
"VideoTimePerSubmission",
"ThreadViewPerSubmission",
"countOfSubmissions"
)
#lm
linearModel <- train(
y=db.train$overalGradeDiff,
x=db.train[,fsNorm],
trControl = ctrl,
metric = "RMSE",
method = "lm"
)
linearModel
#lm
linearModel <- train(
y=db.train$overalGradeDiff,
x=db.train[,fsNorm],
trControl = ctrl,
metric = "RMSE",
method = "lm"
)
linearModel
fs = c(
#"countOfVideoEvents",
"countOfForumEvents",
"ProblemID",
"totalTime",
"totalVideoTime",
#"NoOfVidoesWatched",
"countOfSubmissions",
"countOfThreadViews"
)
model3<- train(y=db.train$overalGradeDiff,
x=db.train[,fs],
method= "cubist",
trControl=ctrl,
tuneGrid = expand.grid(committees = c(21,22,23), neighbors = c(8,9)),
metric="RMSE",
preProc= c("center", "scale"))
model3
fs = c(
"countOfVideoEvents",
#"countOfForumEvents",
"ProblemID",
"totalTime",
"totalVideoTime",
#"NoOfVidoesWatched",
"countOfSubmissions",
"countOfThreadViews"
)
#cubist
model3<- train(y=db.train$overalGradeDiff,
x=db.train[,fs],
method= "cubist",
trControl=ctrl,
#tuneGrid = expand.grid(committees = c(21,22,23), neighbors = c(8,9)),
metric="RMSE",
preProc= c("center", "scale"))
model3
#cubist
model3<- train(y=db.train$overalGradeDiff,
x=db.train[,fsNorm],
method= "cubist",
trControl=ctrl,
#tuneGrid = expand.grid(committees = c(21,22,23), neighbors = c(8,9)),
metric="RMSE",
preProc= c("center", "scale"))
model3
model
model$finalModel
model$trainingData
model$trainingData$NoOfVidoesWatchedPerSubmission
View(model$trainingData$ThreadViewPerSubmission)
#cubist
model3<- train(y=db.train$overalGradeDiff,
x=model$trainingData,
method= "cubist",
trControl=ctrl,
#tuneGrid = expand.grid(committees = c(21,22,23), neighbors = c(8,9)),
metric="RMSE",
preProc= c("center", "scale"))
#cubist
model3<- train(y=model$trainingData$overalGradeDiff,
x=model$trainingData,
method= "cubist",
trControl=ctrl,
#tuneGrid = expand.grid(committees = c(21,22,23), neighbors = c(8,9)),
metric="RMSE",
preProc= c("center", "scale"))
#cubist
model3<- train(y=model$trainingData$.outcomes,
x=model$trainingData,
method= "cubist",
trControl=ctrl,
#tuneGrid = expand.grid(committees = c(21,22,23), neighbors = c(8,9)),
metric="RMSE",
preProc= c("center", "scale"))
View(model$trainingData)
View(db.train[,fsNorm])
library(dplyr)
library(plyr) #ddply
#------ read data frame
db=read.csv('datasets/OutputTable.csv')
#------ sort submissions
db=db[order(db$UserID,db$ProblemID,db$SubmissionNumber),]
# dim(db)
# View(db)
#------- aggregate by UserID and ProblemID ---------
length(unique(db$UserID))
agg.features=ddply(db, .(UserID,ProblemID), summarise,
overalGradeDiff=Grade[length(Grade)]-Grade[1],
countOfSubmissions=length(SubmissionNumber),
totalTime = sum(TimeSinceLast, na.rm = T),
totalVideoTime= sum(DurationOfVideoActivity,na.rm=T),
countOfVideoEvents = sum(NVideoEvents,na.rm = T),
countOfForumEvents = sum(NForumEvents,na.rm = T),
countOfThreadViews = sum(NumberOfThreadViews, na.rm=T),
NoOfVidoesWatched = sum(NoOfVidoesWatched, na.rm = T))
#-------normalize on submissions---------
agg.features["VideoPerSubmission"]<-NA
agg.features$VideoPerSubmission<-(agg.features$countOfVideoEvents/agg.features$countOfSubmissions)
agg.features["ForumPerSubmission"]<-NA
agg.features$ForumPerSubmission<-(agg.features$countOfForumEvents/agg.features$countOfSubmissions)
agg.features["VideoTimePerSubmission"]<-NA
agg.features$VideoTimePerSubmission<-(agg.features$totalVideoTime/agg.features$countOfSubmissions)
agg.features["ThreadViewPerSubmission"]<-NA
agg.features$ThreadViewPerSubmission<-(agg.features$countOfThreadViews/agg.features$countOfSubmissions)
agg.features["NoOfVidoesWatchedPerSubmission"]<-NA
agg.features$NoOfVidoesWatchedPerSubmission <-(agg.features$NoOfVidoesWatched/
agg.features$countOfSubmissions)
#agg.features[is.na(agg.features)] <- 0
#------ remove cases with only one attempt
agg.features=filter(agg.features,countOfSubmissions>1); dim(agg.features)
#------ save feature file
write.csv(agg.features, file='features.csv')
#SAME FOR TEST
#------ read data frame
db=read.csv('datasets/OutputTable_test.csv')
#------ sort submissions
db=db[order(db$UserID,db$ProblemID,db$SubmissionNumber),]
# dim(db)
# View(db)
#------- aggregate by UserID and ProblemID ---------
length(unique(db$UserID))
agg.features=ddply(db, .(UserID,ProblemID), summarise,
overalGradeDiff=Grade[length(Grade)]-Grade[1],
countOfSubmissions=length(SubmissionNumber),
totalTime = sum(TimeSinceLast, na.rm = T),
totalVideoTime= sum(DurationOfVideoActivity,na.rm=T),
countOfVideoEvents = sum(NVideoEvents,na.rm = T),
countOfForumEvents = sum(NForumEvents,na.rm = T),
countOfThreadViews = sum(NumberOfThreadViews, na.rm=T),
NoOfVidoesWatched = sum(NoOfVidoesWatched, na.rm = T))
#-------normalize on submissions---------
agg.features["VideoPerSubmission"]<-NA
agg.features$VideoPerSubmission<-(agg.features$countOfVideoEvents/agg.features$countOfSubmissions)
agg.features["ForumPerSubmission"]<-NA
agg.features$ForumPerSubmission<-(agg.features$countOfForumEvents/agg.features$countOfSubmissions)
agg.features["VideoTimePerSubmission"]<-NA
agg.features$VideoTimePerSubmission<-(agg.features$totalVideoTime/agg.features$countOfSubmissions)
agg.features["ThreadViewPerSubmission"]<-NA
agg.features$ThreadViewPerSubmission<-(agg.features$countOfThreadViews/agg.features$countOfSubmissions)
agg.features["NoOfVidoesWatchedPerSubmission"]<-NA
agg.features$NoOfVidoesWatchedPerSubmission <-(agg.features$NoOfVidoesWatched/
agg.features$countOfSubmissions)
# agg.features[is.na(agg.features)] <- 0
#------ remove cases with only one attempt
agg.features=filter(agg.features,countOfSubmissions>1); dim(agg.features)
#------ save feature file
write.csv(agg.features, file='features_test.csv')
#------ read features extracted from train set, using your python script
db=read.csv('features.csv', stringsAsFactors = F)
#------ sort submissions
db=db[order(db$UserID,db$ProblemID),]
#--- replace NA values with 0
db[is.na(db)]=0
#---- remove cases when there is no video or forum activity between two submissions
db= filter(db, countOfForumEvents + countOfVideoEvents>0)
# ----- (Optional) split your training data into train and test set. Use train set to build your classifier and try it on test data to check generalizability.
set.seed(1234)
tr.index= sample(nrow(db), nrow(db)*0.99)
db.train= db[tr.index,]
db.test = db[-tr.index,]
dim(db.train)
dim(db.test)
library(caret)
fs = c(
"countOfVideoEvents",
#"countOfForumEvents",
"ProblemID",
"totalTime",
"totalVideoTime",
#"NoOfVidoesWatched",
"countOfSubmissions",
"countOfThreadViews"
)
#for normalized features
fsNorm =c(
#"VideoPerSubmission",
#"ForumPerSubmission",
"ProblemID",
"totalTime",
#"NoOfVidoesWatchedPerSubmission",
"VideoTimePerSubmission",
"ThreadViewPerSubmission",
"countOfSubmissions"
)
#check correlation
correlation_matrix <- cor(db.train[,fs])
corrplot(correlation_matrix, method = "number")
#Control function
set.seed(123)
ctrl <- trainControl(method = "repeatedcv", repeats =3, number = 10)
View(db.train[,fsNorm])
fsNorm =c(
#"VideoPerSubmission",
#"ForumPerSubmission",
"ProblemID",
"totalTime",
"NoOfVidoesWatchedPerSubmission",
"VideoTimePerSubmission",
"ThreadViewPerSubmission",
"countOfSubmissions"
)
View(db.train[,fsNorm])
#------ read features extracted from train set, using your python script
db=read.csv('features.csv', stringsAsFactors = F)
#------ sort submissions
db=db[order(db$UserID,db$ProblemID),]
#--- replace NA values with 0
db[is.na(db)]=0
#---- remove cases when there is no video or forum activity between two submissions
db= filter(db, countOfForumEvents + countOfVideoEvents>0)
# ----- (Optional) split your training data into train and test set. Use train set to build your classifier and try it on test data to check generalizability.
set.seed(1234)
tr.index= sample(nrow(db), nrow(db)*0.99)
db.train= db[tr.index,]
db.test = db[-tr.index,]
dim(db.train)
dim(db.test)
library(caret)
fs = c(
"countOfVideoEvents",
"countOfForumEvents",
"ProblemID",
"totalTime",
"totalVideoTime",
"NoOfVidoesWatched",
"countOfSubmissions",
"countOfThreadViews"
)
#for normalized features
fsNorm =c(
#"VideoPerSubmission",
#"ForumPerSubmission",
"ProblemID",
"totalTime",
"NoOfVidoesWatchedPerSubmission",
"VideoTimePerSubmission",
"ThreadViewPerSubmission",
"countOfSubmissions"
)
#Control function
set.seed(123)
ctrl <- trainControl(method = "repeatedcv", number=10, repeats =3)
#lm
model <- train(
y=db.train$overalGradeDiff,
x=db.train[,fs],
metric= "RMSE",
trControl = ctrl,
method = "lm"
)
model
model
