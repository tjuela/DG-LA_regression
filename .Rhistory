setwd("/Users/thomasjuelandersen/Documents/Documents - Thomass MacBook/EPFL/Digital Education and Learning Analytics/Analytics stuff")
d<-read.table("Cars93 Dataset.txt", sep = "\t", header = TRUE)
names(d)
View(d)
View(d)
View(d)
View(d)
d<-read.table("Cars93 Dataset.txt", sep = "\t", header = TRUE)
d<-read.table("Cars93.txt", sep = "\t", header = TRUE)
names(d)
summary(d$Price)
mean($Price)
mean(d$Price)
sd(d$Price)
ci <- (sd(d$Price) / sqrt($len(d$Price))) * 1.96
ci <- (sd(d$Price) / sqrt(d$len(d$Price))) * 1.96
ci <- (sd(d$Price) / sqrt(len(d$Price))) * 1.96
ci <- (sd(d$Price) / sqrt(length(d$Price))) * 1.96
hist(d$Price)
boxplot(d$Price, horizontal = T)
boxplot(d$Price ~ d$Type, horizontal = T)
boxplot(d$Price ~ d$Type, horizontal = F)
library(gplots)
install.packages("gplots")
library("gplots")
plotmeans(d$Price ~ d$Type)
oneway.test(d$Price ~d$Type)
oneway.test(d$Price ~d$Type, var.equal = T)
hist(d$Price)
par(mfrow=(1,2))
par(mfrow=c(1,2))
hist(d$Price)
hist(sqrt(d$Price))
qqnorm(d$Price)
qqnorm(sqrt(d$Price))
qqline
shapiro.test(d$Price)
names(d)
names(d$Manufacturer)
summary(d$Manufacturer)
d$Asian <- factor(ifelse(d$Manufacturer == "Honda" |
d$Manufacturer == "Hyundai" |
d$Manufacturer == "Mazda" |
d$Manufacturer == "Mitsubishi" |
d$Manufacturer == "Nissan" |
d$Manufacturer == "Suzuki" |
, "Asian", "non-Asian") +)
d$Asian <- factor(ifelse(d$Manufacturer == "Honda" |
d$Manufacturer == "Hyundai" |
d$Manufacturer == "Mazda" |
d$Manufacturer == "Mitsubishi" |
d$Manufacturer == "Nissan" |
d$Manufacturer == "Suzuki" |
, "Asian", "non-Asian") +)
d$Asian <- factor(ifelse(d$Manufacturer == "Honda" |
d$Manufacturer == "Hyundai" |
d$Manufacturer == "Mazda" |
d$Manufacturer == "Mitsubishi" |
d$Manufacturer == "Nissan" |
d$Manufacturer == "Suzuki"
, "Asian", "non-Asian") +)
d$Asian <- factor(ifelse(d$Manufacturer == "Honda" |
d$Manufacturer == "Hyundai" |
d$Manufacturer == "Mazda" |
d$Manufacturer == "Mitsubishi" |
d$Manufacturer == "Nissan" |
d$Manufacturer == "Suzuki"
, "Asian", "non-Asian"))
d$Asian <- factor(ifelse(d$Manufacturer == "Honda" |
d$Manufacturer == "Hyundai" |
d$Manufacturer == "Mazda" |
d$Manufacturer == "Mitsubishi" |
d$Manufacturer == "Nissan" |
d$Manufacturer == "Suzuki"
, "Asian", "non-Asian"))
summary(d$Asian)
d$Asian
d$Asian <- factor(ifelse(d$Manufacturer == "Honda" |
d$Manufacturer == "Hyundai" |
d$Manufacturer == "Mazda" |
d$Manufacturer == "Mitsubishi" |
d$Manufacturer == "Nissan" |
d$Manufacturer == "Suzuki" |
, "Asian", "non-Asian"))
d$Asian <- factor(ifelse(d$Manufacturer == "Honda" |
d$Manufacturer == "Hyundai" |
d$Manufacturer == "Mazda" |
d$Manufacturer == "Mitsubishi" |
d$Manufacturer == "Nissan" |
d$Manufacturer == "Suzuki"
, "Asian", "non-Asian"))
d$Asian
summary(d$Asian)
d$Asian <- factor(ifelse(d$Manufacturer == "Honda" |
d$Manufacturer == "Hyundai" |
d$Manufacturer == "Mazda" |
d$Manufacturer == "Mitsubishi" |
d$Manufacturer == "Nissan" |
d$Manufacturer == "Suzuki" |
, "Asian", "non-Asian"))
d$Asian <- factor(ifelse(d$Manufacturer == "Honda" |
d$Manufacturer == "Hyundai" |
d$Manufacturer == "Mazda" |
d$Manufacturer == "Mitsubishi" |
d$Manufacturer == "Nissan" |
d$Manufacturer == "Suzuki" | d$Manufacturer == "Toyota"
, "Asian", "non-Asian"))
summary(d$Asian)
par(mfrow(1,1))
par(mfrow=c(1,1))
plotmeans(d$Price ~ d$Asian)
oneway.test(d$Price ~ d$Asian)
var.test(d$Price ~ d$Asian)
# Load Cars93 dataset.
d <- read.table("Cars93.txt", header=T, sep="\t")
# ex1. Install and load caret package in your R environment.
library(caret)
m2 <- lm(Price ~ Horsepower + Weight, data=d)
cm2 <- train(Price ~ Horsepower+Wheelbase, data=d, method = "lm")
summary(m2)
summary(cm2)
dn = d[,sapply(d, is.numeric)]
View(d)
View(dn)
names(dn)
dn = dn[, -c(1,3)]
# ex6. remove rows with NA values.
dn=na.omit(dn)
# ex7. partition the data into train(70%) and test (30%) sets using createDataPartition().
set.seed(134)
tr.index= createDataPartition(y=dn$Price, p=0.70, list = F)
d.train= dn[tr.index,]
d.test = dn[-tr.index,]
# ex8. On the training set, build a linear model of the Price, using all the numerical features.
cm3 <- train(Price ~ ., data=d.train, method = "lm")
# ex9. How much variance does the model explain?
summary(cm3)$r.squared
# ex10. Predict the price for the samples in the test set.
pred3=predict(cm3, newdata=d.test)
# ex11. plot the predicted and observed valuse in different colors.
plot(d.test$Price, col='blue', ylim=c(0, max(d.test$Price,pred3)));
points(pred3, col='red')
# ex12. Define a function which computes MSE
#MSE =  function(Y,Yhat) { mean((Y - Yhat)^2) }
RMSE = function(Y,Yhat){ sqrt(mean((Y - Yhat)^2)) }
# ex13. Use your RMSE function to compute the prediction error (between predicted and observed prices).
(RMSE(d.test$Price, pred3))
# ex14. Use method='lmStepAIC' to simplify the model by removing features of low importance basd on AIC criteria. Compare the variance explained by the reduce model and the saturated model.
cm4 <- train(Price ~ ., data=d.train, method = "lmStepAIC")
summary(cm4)$r.squared
# ex15. use the reduced model to predict the price for the samples in the test set. Compare the prediction error between the reduce and saturated models
pred4=predict(cm4, newdata=d.test)
(RMSE(d.test$Price, pred4))
# ex16. On the training set, build a linear svm (method='svmLinear') model of the Price, using 'Horsepower', 'Wheelbase', 'Luggage.room' features.
# Apply the model on test data and compute the prediction error.
fs= c('Horsepower', 'Wheelbase', 'Luggage.room')
svm1=train(x=d.train[,fs] , y=d.train$Price, method = "svmLinear")
plot(varImp(svm1))
pred=predict(svm1, newdata=d.test[,fs])
(RMSE(d.test$Price, pred))
#ex17. Use cross validation as the resampling method in the training phase (trControl parameter in train function).
ctrl= trainControl(method = 'cv')
svm2=train(x=d.train[,fs] , y=d.train$Price, method = "svmLinear",
trControl = ctrl)
#ex18. Tune the model parameter using tuneGrid parameter. Plot the result.
paramGrid <- expand.grid(C = c(0.001,0.01,0.1,0.5 ,1, 2, 3, 4))
svm3=train(x=d.train[,fs] , y=d.train$Price, method = "svmLinear",
trControl = ctrl,
tuneGrid = paramGrid )
svm3
plot(svm3)
plot(varImp(svm3))
pred=predict(svm3, newdata=d.test[,fs])
(RMSE(d.test$Price, pred))
#ex19. try other models in caret such as
# - neural networks (nnet)
# - Random forests (rf)
#================================== Classification in Caret ===============
#ex20. Use median cut on 'Price' to add a new fetaue 'PriceCategory'with two levels 'high' and 'low' to your train and test sets.
d.train$PriceCategory <- factor(ifelse(d.train$Price > median(d.train$Price),"High","Low"))
d.test$PriceCategory <- factor(ifelse(d.test$Price > median(d.test$Price),"High","Low"))
#ex21.  Use featurePlot to visualize the previous feature set for the PriceCategory classes in your train set.
featurePlot(x = d.train[,fs], y = d$PriceCategory,
plot = "pairs", # scatter plots
auto.key = list(columns = 2) # Add a key at the top
)
#ex22. use the previous setup to train a SVM classifier for predicting the PriceCategory.
fs= c('Horsepower', 'Wheelbase', 'Luggage.room')
ctrl= trainControl(method = 'cv')
paramGrid <- expand.grid(C = c(0.001,0.01,0.1,0.5 ,1, 2, 3, 4))
svmC1=train(x=d.train[,fs] , y=d.train$PriceCategory, method = "svmLinear",
trControl = ctrl,
tuneGrid = paramGrid )
svmC1
plot(svmC1)
# ex23. Predic the PriceCategory for the test data and evaluate the prediction accuracy using confusionMatrix method.
pred=predict(svmC1, newdata=d.test[,fs])
confusionMatrix(pred, d.test$PriceCategory)
c(1, 7:9)
c(7:9)
l = 10
c(l-1:l+1)
c((l-1):(l+1))
install.packages('AUC')
#Gaussian Process with Radial Basis Function Kernel
gmr <- train(y=db.train$overalGradeDiff,
x=db.train[,fs],
method= "gaussprRadial",
trControl=crtl,
metric="RMSE",
preProc= c("center", "scale"))
library(dplyr)
library(plyr) #ddply
library(caret)
#========================================================================
#         step 1: train classifier
#========================================================================
#------ read features extracted from train set, using your python script
db=read.csv('features.csv', stringsAsFactors = F)
#------ sort submissions
db=db[order(db$UserID,db$ProblemID),]
#--- replace NA values with 0
db[is.na(db)]=0
#---- remove cases when there is no video or forum activity between two submissions
db= filter(db, countOfForumEvents + countOfVideoEvents>0)
# ----- (Optional) split your training data into train and test set. Use train set to build your classifier and try it on test data to check generalizability.
set.seed(1234)
tr.index= sample(nrow(db), nrow(db)*0.9)
db.train= db[tr.index,]
db.test = db[-tr.index,]
dim(db.train)
dim(db.test)
library(caret)
fs = c(
"countOfVideoEvents",
"countOfForumEvents",
"ProblemID",
"totalTime",
"totalVideoTime",
"countOfSubmissions"
)
#Gaussian Process with Radial Basis Function Kernel
gmr <- train(y=db.train$overalGradeDiff,
x=db.train[,fs],
method= "gaussprRadial",
trControl=crtl,
metric="RMSE",
preProc= c("center", "scale"))
# ----- (Optional) split your training data into train and test set. Use train set to build your classifier and try it on test data to check generalizability.
set.seed(1234)
tr.index= sample(nrow(db), nrow(db)*0.9)
db.train= db[tr.index,]
db.test = db[-tr.index,]
dim(db.train)
dim(db.test)
#------ read features extracted from train set, using your python script
db=read.csv('features.csv', stringsAsFactors = F)
#------ sort submissions
db=db[order(db$UserID,db$ProblemID),]
#--- replace NA values with 0
db[is.na(db)]=0
setwd("~/Documents/EPFL/Digital Education and Learning Analytics/DG&LA_regression")
library(dplyr)
library(plyr) #ddply
library(caret)
#========================================================================
#         step 1: train classifier
#========================================================================
#------ read features extracted from train set, using your python script
db=read.csv('features.csv', stringsAsFactors = F)
#------ sort submissions
db=db[order(db$UserID,db$ProblemID),]
#--- replace NA values with 0
db[is.na(db)]=0
#---- remove cases when there is no video or forum activity between two submissions
db= filter(db, countOfForumEvents + countOfVideoEvents>0)
# ----- (Optional) split your training data into train and test set. Use train set to build your classifier and try it on test data to check generalizability.
set.seed(1234)
tr.index= sample(nrow(db), nrow(db)*0.9)
db.train= db[tr.index,]
db.test = db[-tr.index,]
dim(db.train)
dim(db.test)
library(caret)
fs = c(
"countOfVideoEvents",
"countOfForumEvents",
"ProblemID",
"totalTime",
"totalVideoTime",
"countOfSubmissions"
)
#for normalized features
fsNorm =c(
"VideoPerSubmission",
"ForumPerSubmission",
"ProblemID",
"totalTime",
"VideoTimePerSubmission",
"countOfSubmissions"
)
#Control function
set.seed(123)
ctrl <- trainControl(method = "repeatedcv", repeats =3)
#Gaussian Process with Radial Basis Function Kernel
gmr <- train(y=db.train$overalGradeDiff,
x=db.train[,fs],
method= "gaussprRadial",
trControl=crtl,
metric="RMSE",
preProc= c("center", "scale"))
#Gaussian Process with Radial Basis Function Kernel
gmr <- train(y=db.train$overalGradeDiff,
x=db.train[,fs],
method= "gaussprRadial",
trControl=ctrl,
metric="RMSE",
preProc= c("center", "scale"))
testDb=read.csv('features_test.csv', stringsAsFactors = F)
testDb$OveralGradeDiff=NULL
testDb[is.na(testDb)]=0
#---- use trained model to predict progress for test data
preds= predict(gmr$finalModel, newdata=testDb[,fs]);
#========================================================================
#         step 2.1: prepare submission file for kaggle
#========================================================================
cl.Results=testDb[,c('ProblemID', 'UserID')]
cl.Results$overalGradeDiff=preds
cl.Results$uniqRowID= paste0(cl.Results$UserID,'_', cl.Results$ProblemID)
cl.Results=cl.Results[,c('uniqRowID','overalGradeDiff')]
table(cl.Results$overalGradeDiff)
#----- keep only rows which are listed in classifier_templtae.csv file
#----- this excludes first submissions and cases with no forum and video event in between two submissions
regression_template= read.csv('regression_template.csv', stringsAsFactors = F)
kaggleSubmission=merge(regression_template,cl.Results )
write.csv(kaggleSubmission,file='regression_results.csv', row.names = F)
View(kaggleSubmission)
